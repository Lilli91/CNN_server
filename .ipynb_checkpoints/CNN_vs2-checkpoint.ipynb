{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Intensity curve shape =', (990, 900))\n",
      "('Classes shape =', (990,))\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import keras\n",
    "from keras.layers import convolutional, Dense, Activation,pooling\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import cPickle as pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "\n",
    "###################################### LOADING DATA ##############################\n",
    "\n",
    "### Create a dictionary that contains all the dictiories within each dataset (workspace) we want to load ###\n",
    "#Go to the data directory\n",
    "dataDir = \"/scratch/barbieri/DATA_CNN_networks/Data/\"\n",
    "Load_Data = {}  \n",
    "for i in range(0, len(os.listdir( dataDir ))):  \n",
    "    Load_Data[i] = {}  #Dictionary for a single workspace\n",
    "\n",
    "    \n",
    "### lOAD THE DATA STORING THEM IN A DICTIONARY ###\n",
    "i = 0\n",
    "for file in os.listdir( dataDir ):\n",
    "    Load_Data[i]['File'] =  scipy.io.loadmat( dataDir+file )\n",
    "    i = i+1\n",
    "\n",
    "    \n",
    "#### Define another dictionary for storing intensity curves and classes ### \n",
    "#From each dataset, the intensity curve and classes are extracted and stored in another dictionary\n",
    "DATA = {}\n",
    "N_of_file =len(os.listdir( dataDir ))\n",
    "for i in range(0, N_of_file):\n",
    "        DATA[i] = {}\n",
    "\n",
    "\n",
    "#Find the minimun length among all the intensity curves\n",
    "Int_curve_lengths = np.zeros(N_of_file)\n",
    "for i in range(0, N_of_file):\n",
    "    Int_curve_lengths [i] = (len(Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0][0]))\n",
    "Int_curve_len = (min(Int_curve_lengths.astype(int)))\n",
    "\n",
    "\n",
    "#### Fill the dictionary ###\n",
    "Ncell_classified = 200 #Number of classified cells per dataset\n",
    "for i in range(0, N_of_file):\n",
    "    #Decision array for each dataset\n",
    "    DATA[i]['decision'] = Load_Data[i]['File']['decisionlist']   \n",
    "    #Size of the matrix to store intensity values\n",
    "    NumofData = len(Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'])\n",
    "    #Empty matrix to store all the intensity values of all the cells\n",
    "    DATA[i]['intensity'] = np.zeros((Ncell_classified,  Int_curve_len ))    \n",
    "    for k in range(0,Ncell_classified): #Only the first 200 were classified\n",
    "        DATA[i]['intensity'][k,:] = Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][k][0,0:900] #copy the first 900 values\n",
    "    \n",
    "\n",
    "### Classes array: contain values from all the data ###\n",
    "Classes=[]\n",
    "for i in range( 0, N_of_file ):\n",
    "    Classes = np.append(Classes , DATA[i]['decision']).astype(int)\n",
    "\n",
    "\n",
    "### Intensities matrix: contain values from all the data    \n",
    "NTot_Data = N_of_file*Ncell_classified\n",
    "Intensity = np.zeros((0, DATA[0]['intensity'].shape[1]))\n",
    "for i in range( 0, N_of_file ):\n",
    "    Intensity = np.append(Intensity, DATA[i]['intensity'][:,:], axis=0)    \n",
    "    \n",
    "\n",
    "### ELIMINATE intensity profiles with NaN values and corresponding classes ###\n",
    "nan_indices = np.where(np.isnan(Intensity))\n",
    "Intensity = np.delete(Intensity,nan_indices[0], axis = 0)\n",
    "Classes = np.delete(Classes,nan_indices[0], axis = 0)\n",
    "print('Intensity curve shape =', Intensity.shape)\n",
    "print('Classes shape =', Classes.shape)\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# DUPLICATE Intensity profiles adding noise to the second group\n",
    "print(total)\n",
    "Intensity_more_curves_noise = np.zeros((2*total,  Int_curve_len ))\n",
    "Intensity_more_curves_noise[0:990,:] = Intensity\n",
    "Intensity_more_curves_noise[990:1980,:] = Intensity + np.random.randn(Int_curve_len)*0.02\n",
    "\n",
    "print(Intensity_more_curves_noise.shape)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.title('Calcium signalling without and with noise added')\n",
    "plot(Intensity_more_curves_noise[1])\n",
    "plt.subplot(212)\n",
    "plot(Intensity_more_curves_noise[991])\n",
    "plt.xlabel('Frame')\n",
    "plt.savefig('13_06_2017_Curve_VS_noiseCurve.png')\n",
    "plt.show()\n",
    "\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity_more_curves_noise)\n",
    "print(total)\n",
    "\n",
    "\n",
    "#DUPLICATE CLASSES\n",
    "#DUPLICATE THEM\n",
    "Classes = np.append(Classes, Classes)\n",
    "print(Classes.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data shape', (990, 256))\n"
     ]
    }
   ],
   "source": [
    "### Downsampling\n",
    "data1 = Intensity[:,0:768]\n",
    "step = 3\n",
    "#256 points\n",
    "data_downsampl = data1[:,::step]\n",
    "print('data shape', data_downsampl.shape)\n",
    "\n",
    "#Redefine len of the Intensity curves:\n",
    "Int_profile_len = len(data_downsampl[0])\n",
    "\n",
    "\n",
    "#Check if there are typo errors for the classes (we only have 6 classes going from 0 to 5)\n",
    "for i in range(len(Classes)):\n",
    "    if Classes[i] > 5:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a CNN\n",
    "\n",
    "def CNN(X, y, test_dt, test_cl, decay_val, n_iteration):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Convolution2D(128, 3, 1,border_mode=\"same\",activation=\"relu\",input_shape=(Int_profile_len,1, 1)))\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "    cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(1024, activation=\"relu\"))\n",
    "    cnn.add(Dropout(0.3)) #0.5\n",
    "    cnn.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "    \n",
    "    # define optimizer and objective, compile cnn\n",
    "    compile_step = cnn.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay_val),  metrics=['accuracy'])\n",
    "    #default parameters coming from the paper: https://arxiv.org/abs/1412.6980v8\n",
    "       \n",
    "    #Store in dictionaries\n",
    "    results = {}\n",
    "    out = {}  \n",
    "    # train\n",
    "    \n",
    "    filepath =\"/scratch/dwaithe/13_06_2017_DropOut03_data_weights_1980Data_best_DecayValue_\" + str(decay_val) + \"NIteration\" + str(n_iteration) + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "    # Fit the model\n",
    "    results = cnn.fit(X, y, nb_epoch=200, batch_size=100, callbacks=callbacks_list, validation_data=(test_dt , test_cl), verbose=0) \n",
    "    #Evaluation code which makes prediction\n",
    "    out = cnn.predict(test_dt.reshape((N_test_cell, Int_profile_len,1, 1)))    \n",
    "    \n",
    "    return results, out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iteration number =', 0)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[8192,1024]\n\t [[Node: random_uniform_18/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=178137469, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_uniform_18/shape)]]\n\nCaused by op u'random_uniform_18/RandomUniform', defined at:\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/t1-home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-8caff8122383>\", line 63, in <module>\n    RESULTS[iteration][i] = CNN(X, y, test_dt_reshape, test_cl_reshape, decay_val[i], iteration)\n  File \"<ipython-input-6-81eafb803892>\", line 20, in CNN\n    cnn.add(Dense(1024, activation=\"relu\"))\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/models.py\", line 327, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 543, in __call__\n    self.build(input_shapes[0])\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/layers/core.py\", line 752, in build\n    constraint=self.W_constraint)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 415, in add_weight\n    weight = initializer(shape, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/initializations.py\", line 60, in glorot_uniform\n    return uniform(shape, s, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/initializations.py\", line 33, in uniform\n    return K.random_uniform_variable(shape, -scale, scale, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 619, in random_uniform_variable\n    low, high, dtype=tf_dtype, seed=seed)(shape)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 171, in _initializer\n    return random_ops.random_uniform(shape, minval, maxval, dtype, seed=seed)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 245, in random_uniform\n    seed2=seed2)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 220, in _random_uniform\n    seed=seed, seed2=seed2, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[8192,1024]\n\t [[Node: random_uniform_18/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=178137469, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_uniform_18/shape)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8caff8122383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecay_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#print('decay value = ', decay_val[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mRESULTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dt_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cl_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-81eafb803892>\u001b[0m in \u001b[0;36mCNN\u001b[0;34m(X, y, test_dt, test_cl, decay_val, n_iteration)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_cl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;31m#Evaluation code which makes prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_test_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInt_profile_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   1603\u001b[0m                               feed_dict=feed_dict)\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'variables_initializer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[8192,1024]\n\t [[Node: random_uniform_18/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=178137469, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_uniform_18/shape)]]\n\nCaused by op u'random_uniform_18/RandomUniform', defined at:\n  File \"/usr/lib64/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib64/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/t1-home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-8caff8122383>\", line 63, in <module>\n    RESULTS[iteration][i] = CNN(X, y, test_dt_reshape, test_cl_reshape, decay_val[i], iteration)\n  File \"<ipython-input-6-81eafb803892>\", line 20, in CNN\n    cnn.add(Dense(1024, activation=\"relu\"))\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/models.py\", line 327, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 543, in __call__\n    self.build(input_shapes[0])\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/layers/core.py\", line 752, in build\n    constraint=self.W_constraint)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/topology.py\", line 415, in add_weight\n    weight = initializer(shape, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/initializations.py\", line 60, in glorot_uniform\n    return uniform(shape, s, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/initializations.py\", line 33, in uniform\n    return K.random_uniform_variable(shape, -scale, scale, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 619, in random_uniform_variable\n    low, high, dtype=tf_dtype, seed=seed)(shape)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/ops/init_ops.py\", line 171, in _initializer\n    return random_ops.random_uniform(shape, minval, maxval, dtype, seed=seed)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 245, in random_uniform\n    seed2=seed2)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 220, in _random_uniform\n    seed=seed, seed2=seed2, name=name)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[8192,1024]\n\t [[Node: random_uniform_18/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=178137469, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](random_uniform_18/shape)]]\n"
     ]
    }
   ],
   "source": [
    "#Iterate the process \n",
    "decay_val = [0.01]#, 0.005, 0.001]\n",
    "n_interation = 5\n",
    "\n",
    "#Matrix to store test_cl and test_dt - DO IT\n",
    "test_dt = np.zeros(((190,256,5)))  #180 data, 256 points for each data, 5 iterations\n",
    "test_cl = np.zeros((5,190))\n",
    "\n",
    "#for iteration in range(n_interation):\n",
    " #   test_dt[iteration] = {}\n",
    "  #  test_cl[iteration] = {}\n",
    "    \n",
    "\n",
    "#dictionary to store the iterations\n",
    "RESULTS = {} \n",
    "for iteration in range(n_interation):\n",
    "    RESULTS[iteration] = {} #dictionary to store the results from the CNN\n",
    "    \n",
    "for iteration in range(n_interation):\n",
    "    print('Iteration number =', iteration)\n",
    "    \n",
    "    ################### Distribute all values randomly ##################\n",
    "    randseq = np.random.choice(np.arange(0,total),total,replace=False)\n",
    "    #Data randomly organized\n",
    "    data_downsampl = data_downsampl[randseq,:]\n",
    "\n",
    "    Classes = Classes[randseq]    \n",
    "    Classes_to_categorical = np_utils.to_categorical(Classes)\n",
    "\n",
    "    ######################### load file - training data ####################\n",
    "    train_dt = data_downsampl[0:800,:]\n",
    "    train_cl = Classes[0:800]\n",
    "    train_cl_reshape = Classes_to_categorical[0:800]\n",
    "    \n",
    "    X = np.copy(train_dt)#np.random.rand(10000, 128).astype(\"float32\")\n",
    "    y = np.copy(train_cl_reshape) #np.random.randint(3, size=(10000,1))\n",
    "    \n",
    "    N_train_cell = len(train_dt)\n",
    "    # process the data to fit in a keras CNN properly\n",
    "    # input data needs to be (N, C, X, Y) - shaped where\n",
    "    # N - number of samples\n",
    "    # C - number of channels per sample\n",
    "    # (X, Y) - sample size\n",
    "    X = X.reshape((N_train_cell, Int_profile_len,1, 1))\n",
    "\n",
    "    ######################### load file - testing data ####################\n",
    "    #Define variables \n",
    "    N_test_cell = total - N_train_cell\n",
    "    \n",
    "    #Matrix for saving values\n",
    "    test_dt[:,:, iteration]= data_downsampl[N_train_cell:total]\n",
    "    test_dt_reshape = np.copy(test_dt[:,:,iteration])\n",
    "    test_dt_reshape = test_dt_reshape.reshape((N_test_cell, Int_profile_len,1, 1))\n",
    "    \n",
    "    #Same process for the associated classes\n",
    "    test_cl[iteration,:] = Classes[N_train_cell:total]\n",
    "    test_cl_reshape = Classes_to_categorical[N_train_cell:total]\n",
    "    \n",
    "    \n",
    "    #RUN THE CNN network\n",
    "    for i in range(len(decay_val)):\n",
    "        #print('decay value = ', decay_val[i])\n",
    "        RESULTS[iteration][i] = CNN(X, y, test_dt_reshape, test_cl_reshape, decay_val[i], iteration)\n",
    "        \n",
    "        #ERROR\n",
    "        #Saves data out using pickle. Will be file named data.p in the same directory as this notebook.\n",
    "        #pickle.dump(RESULTS[i], open( \"Results_CNN_Different_Decay\"  + str(k) + \".p\", \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save LOSS and ACCURACY curve in txt files\n",
    "\n",
    "#SAVE LOSS\n",
    "for iteration in range(n_interation):\n",
    "    for i in range(len(decay_val)):\n",
    "        np.savetxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_LossValues_Diff_decay_Niter_\"+ str(iteration)+ \"_DecVal_\" + str(decay_val[i]) + \".txt\", RESULTS[iteration][i][0].history['loss'], newline='\\r\\n')\n",
    "\n",
    "#SAVE ACCURACY\n",
    "for iteration in range(n_interation):\n",
    "    for i in range(len(decay_val)):\n",
    "        np.savetxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_AccValues_Diff_decay_Niter_\"+ str(iteration)+ \"_DecVal_\" + str(decay_val[i]) + \".txt\", RESULTS[iteration][i][0].history['acc'], newline='\\r\\n')\n",
    "\n",
    "#SAVE VAL_LOSS\n",
    "for iteration in range(n_interation):\n",
    "    for i in range(len(decay_val)):\n",
    "        np.savetxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_Validation_LossValues_Diff_decay_Niter_\"+ str(iteration)+ \"_DecVal_\" + str(decay_val[i]) + \".txt\", RESULTS[iteration][i][0].history['val_loss'], newline='\\r\\n')\n",
    "\n",
    "#SAVE VAL_ACC\n",
    "for iteration in range(n_interation):\n",
    "    for i in range(len(decay_val)):\n",
    "        np.savetxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_Validation_AccValues_Diff_decay_Niter_\"+ str(iteration)+ \"_DecVal_\" + str(decay_val[i]) + \".txt\", RESULTS[iteration][i][0].history['val_acc'], newline='\\r\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot LOSS and val_loss - all decay\n",
    "loss = {}\n",
    "val_loss = {}\n",
    "for k in range(len(decay_val)):\n",
    "    loss[k] = np.zeros((200,n_interation))\n",
    "    val_loss[k] = np.zeros((200,n_interation))\n",
    "\n",
    "loss_mean = np.zeros((200,len(decay_val)))  #rows = n. of epochs; columns = n. of decay values\n",
    "val_loss_mean = np.zeros((200,len(decay_val)))\n",
    "\n",
    "for k in range(len(decay_val)):   #scan along all the decay values\n",
    "    for iteration in range(n_interation): #scan along all the iterations\n",
    "        loss[k][:,iteration] = np.loadtxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_LossValues_Diff_decay_Niter_\" + str(iteration) + \"_DecVal_\" + str(decay_val[k]) + \".txt\")\n",
    "        val_loss[k][:,iteration] = np.loadtxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_Validation_LossValues_Diff_decay_Niter_\" + str(iteration) + \"_DecVal_\" + str(decay_val[k]) + \".txt\")\n",
    "\n",
    "        #AVERAGE OF THE loss\n",
    "        loss_mean[:,k] = np.mean(loss[k], axis = 1) \n",
    "        #AVERAGE OF THE VAL_Loss\n",
    "        val_loss_mean[:,k] = np.mean(val_loss[k], axis = 1)  \n",
    "        \n",
    "\n",
    "print(loss_mean.shape)\n",
    "print(val_loss_mean.shape)\n",
    "#PLOT\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(decay_val)):\n",
    "    ax.plot(loss_mean[:,i],'-', linewidth=2, label = 'Loss'+'Decay'+str(decay_val[i]))\n",
    "    ax.plot(val_loss_mean[:,i],'o', ms=1, label = 'val_loss'+'Decay'+str(decay_val[i]))\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_title('Average LOSS and VAL_LOSS - 5 iterations - 200Epochs - Dropout02')\n",
    "legend = plt.legend(loc='upper right', shadow=False, prop={'size':10})\n",
    "fig.savefig('/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_LOSS_and_VAL_Loss_5iterations_Epoches200_decay01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot ACC and val_acc - Decay 0.005\n",
    "acc = {}\n",
    "val_acc = {}\n",
    "\n",
    "\n",
    "for k in range(len(decay_val)):\n",
    "    acc[k] = np.zeros((200,n_interation))\n",
    "    val_acc[k] = np.zeros((200,n_interation))\n",
    "    \n",
    "acc_mean = np.zeros((200,len(decay_val)))  #rows = n. of epochs; columns = n. of decay values\n",
    "val_acc_mean = np.zeros((200,len(decay_val)))\n",
    "\n",
    "for k in range(len(decay_val)):\n",
    "    for iteration in range(n_interation):\n",
    "        acc[k][:,iteration] = np.loadtxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_AccValues_Diff_decay_Niter_\" + str(iteration) + \"_DecVal_\" + str(decay_val[k]) + \".txt\")\n",
    "        val_acc[k][:,iteration] = np.loadtxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_Validation_AccValues_Diff_decay_Niter_\" + str(iteration) + \"_DecVal_\" + str(decay_val[k]) + \".txt\")\n",
    "\n",
    "        for epoch in range(200):\n",
    "            #AVERAGE OF THE ACC\n",
    "            acc_mean[:,k] = np.mean(acc[k], axis = 1) \n",
    "            #AVERAGE OF THE VAL_ACC\n",
    "            val_acc_mean[:,k] = np.mean(val_acc[k], axis = 1)  \n",
    "\n",
    "        \n",
    "print(acc_mean.shape)\n",
    "print(val_acc_mean.shape)\n",
    "#PLOT\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(decay_val)):\n",
    "    ax.plot(acc_mean[:,i], '-', linewidth=2, label = 'Acc'+'Decay'+str(decay_val[i]))\n",
    "    ax.plot(val_acc_mean[:,i],'o', ms=2, label = 'val_acc'+'Decay'+str(decay_val[i]) )\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_title('Average ACC and VAL_ACC - 5 iterations - 200Epochs - dropout02')\n",
    "legend = plt.legend(loc='4', shadow=False, prop={'size':10})\n",
    "fig.savefig('/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_ACC_and_VAL_Acc_5iterations_Epoches200_decay01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentage = np.zeros(len(decay_val)*n_interation)\n",
    "k=0\n",
    "for i in range(len(decay_val)):\n",
    "    for iteration in range(n_interation):\n",
    "    #print('Iteration = ', iteration)\n",
    "        #print('Decay value =', decay_val[i])\n",
    "        percentage[k] = float(np.sum(np.argmax(RESULTS[iteration][i][1],1)==test_cl[iteration]))/float(test_cl[iteration].shape[0])*100.0\n",
    "        #print('Prediction percentage', percentage)\n",
    "        k=k+1\n",
    "print(percentage)\n",
    "np.savetxt(\"/scratch/dwaithe/13_06_2017_CNN2_DropOut02_800Data_Prediction_percentage_NEW.txt\", percentage,  fmt='%.4e', newline='\\r\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
