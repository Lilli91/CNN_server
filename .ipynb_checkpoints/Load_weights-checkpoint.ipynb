{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n",
      "(1980,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import keras\n",
    "from keras.layers import convolutional, Dense, Activation,pooling\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import cPickle as pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "\n",
    "###################################### LOADING DATA ##############################\n",
    "\n",
    "### Create a dictionary that contains all the dictiories within each dataset (workspace) we want to load ###\n",
    "\n",
    "#Go to the data directory\n",
    "dataDir = \"/scratch/barbieri/DATA_CNN_networks/Data/\"\n",
    "Load_Data = {}  \n",
    "for i in range(0, len(os.listdir( dataDir ))):  \n",
    "    Load_Data[i] = {}  #Dictionary for a single workspace\n",
    "\n",
    "    \n",
    "### lOAD THE DATA STORING THEM IN A DICTIONARY ###\n",
    "i = 0\n",
    "for file in os.listdir( dataDir ):\n",
    "    Load_Data[i]['File'] =  scipy.io.loadmat( dataDir+file )\n",
    "    i = i+1\n",
    "\n",
    "    \n",
    "#### Define another dictionary for storing intensity curves and classes ### \n",
    "#From each dataset, the intensity curve and classes are extracted and stored in another dictionary\n",
    "DATA = {}\n",
    "N_of_file =len(os.listdir( dataDir ))\n",
    "for i in range(0, N_of_file):\n",
    "        DATA[i] = {}\n",
    "\n",
    "\n",
    "#### Fill the dictionary ###\n",
    "Ncell_classified = 200 #Number of classified cells per dataset\n",
    "for i in range(0, N_of_file):\n",
    "    #Decision array for each dataset\n",
    "    DATA[i]['decision'] = Load_Data[i]['File']['decisionlist']\n",
    "    #Size of the matrix to store intensity values\n",
    "    NumofData = len(Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'])\n",
    "    Int_curve_len = len(Load_Data[4]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0][0])\n",
    "    #Empty matrix to store all the intensity values of all the cells\n",
    "    DATA[i]['intensity'] = np.zeros((Ncell_classified,  Int_curve_len ))\n",
    "    for k in range(0,Ncell_classified): #Only the first 200 were classified\n",
    "        DATA[i]['intensity'][k,:] = Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][k][0,0:900] #copy the first 900 values\n",
    "\n",
    "        \n",
    "### Classes array: contain values from all the data ###\n",
    "Classes=[]\n",
    "for i in range( 0, N_of_file ):\n",
    "    Classes = np.append(Classes , DATA[i]['decision']).astype(int)\n",
    "\n",
    "    \n",
    "### Intensities matrix: contain values from all the data    \n",
    "NTot_Data = N_of_file*Ncell_classified\n",
    "Intensity = np.zeros((0, DATA[0]['intensity'].shape[1]))\n",
    "for i in range( 0, N_of_file ):\n",
    "    Intensity = np.append(Intensity, DATA[i]['intensity'][:,:], axis=0)\n",
    "\n",
    "    \n",
    "### Elimate intensity profiles with NaN values and corresponding classes ###\n",
    "nan_indices = np.where(np.isnan(Intensity))\n",
    "Intensity = np.delete(Intensity,nan_indices[0], axis = 0)\n",
    "Classes = np.delete(Classes,nan_indices[0], axis = 0)\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity)\n",
    "\n",
    "# DUPLICATE Intensity profiles adding noise to the second group\n",
    "print(total)\n",
    "Intensity_more_curves_noise = np.zeros((2*total,  Int_curve_len ))\n",
    "Intensity_more_curves_noise[0:990,:] = Intensity\n",
    "Intensity_more_curves_noise[990:1980,:] = Intensity + np.random.randn(Int_curve_len)*0.01\n",
    "\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity_more_curves_noise)\n",
    "\n",
    "\n",
    "#DUPLICATE CLASSES\n",
    "Classes = np.append(Classes, Classes)\n",
    "print(Classes.shape)\n",
    "\n",
    "\n",
    "### Downsampling\n",
    "data1 = Intensity_more_curves_noise[:,0:768]\n",
    "step = 3\n",
    "#256 points\n",
    "data_downsampl = data1[:,::step]\n",
    "#Redefine len of the Intensity curves:\n",
    "Int_profile_len = len(data_downsampl[0])\n",
    "\n",
    "#Check if there are typo errors for the classes (we only have 6 classes going from 0 to 5)\n",
    "for i in range(len(Classes)):\n",
    "    if Classes[i] > 5:\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "### Distribute all values randomly ###\n",
    "randseq = np.random.choice(np.arange(0,total),total,replace=False)\n",
    "#Data randomly organized\n",
    "data_downsampl = data_downsampl[randseq,:]\n",
    "Classes = Classes[randseq]\n",
    "Classes_to_categorical = np_utils.to_categorical(Classes)\n",
    "\n",
    "######################### load file - training data ####################\n",
    "train_dt = data_downsampl[0:800,:]\n",
    "train_cl = Classes_to_categorical[0:800]\n",
    "X = train_dt  \n",
    "y = train_cl \n",
    "N_train_cell = len(train_dt)\n",
    "# process the data to fit in a keras CNN properly\n",
    "# input data needs to be (N, C, X, Y) - shaped where\n",
    "# N - number of samples\n",
    "# C - number of channels per sample\n",
    "# (X, Y) - sample size\n",
    "\n",
    "X = X.reshape((N_train_cell, Int_profile_len,1, 1))\n",
    "\n",
    "\n",
    "######################### load file - testing data ####################\n",
    "#Define variables \n",
    "N_test_cell = total - N_train_cell\n",
    "#Matrix for intensity values\n",
    "test_dt = np.zeros((N_test_cell,Int_profile_len))\n",
    "#Use as testing data the last N_test_cell values within the total \n",
    "test_dt[:,:]= data_downsampl[N_train_cell:total]\n",
    "#Same process for the associated classes\n",
    "test_cl = Classes_to_categorical[N_train_cell:total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# How to load and use weights from a checkpoint\n",
    "\n",
    "# define a CNN\n",
    "# see http://keras.io for API reference\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(64, 3, 1,border_mode=\"same\",activation=\"relu\",input_shape=(Int_profile_len,1, 1)))\n",
    "cnn.add(Convolution2D(64, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution2D(128, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(Convolution2D(256, 3, 1, border_mode=\"same\", activation=\"relu\"))\n",
    "cnn.add(MaxPooling2D(pool_size=(2,1)))\n",
    "    \n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(1024, activation=\"relu\"))\n",
    "cnn.add(Dropout(0.5))\n",
    "cnn.add(Dense(6, activation=\"softmax\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n"
     ]
    }
   ],
   "source": [
    "# load weights\n",
    "cnn.load_weights(\"/scratch/barbieri/DATA_CNN_networks/Different_decay_val/iteration1/06_06_2017_Noise1800data_weights.best_DecayValue0.001Iter0.hdf5\")\n",
    "# Compile model (required to make predictions)\n",
    "#cnn.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.001), metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ecd653748342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;31m# prepare inputs, delegate logic to _test_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/molimm2/dwaithe/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    964\u001b[0m                                check_batch_dim=True, batch_size=None):\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    967\u001b[0m                                \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                                'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "scores = cnn.evaluate(test_dt, test_cl, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180, 6)\n",
      "[2 1 2 ..., 4 4 2]\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on whole dataset using loaded weights\n",
    "out = cnn.predict(test_dt.reshape((N_test_cell, Int_profile_len,1, 1)))  \n",
    "\n",
    "print(out.shape)\n",
    "print(np.argmax(out,1))\n",
    "#print((float(np.sum(np.argmax(out,0)==test_cl))/float(test_cl.shape[0])*100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
