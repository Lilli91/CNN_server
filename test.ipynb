{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import keras\n",
    "from keras.layers import convolutional, Dense, Activation,pooling\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Flatten, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import cPickle as pickle\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "\n",
    "###################################### LOADING DATA ##############################\n",
    "\n",
    "### Create a dictionary that contains all the dictiories within each dataset (workspace) we want to load ###\n",
    "\n",
    "#Go to the data directory\n",
    "dataDir = \"/scratch/barbieri/DATA_CNN_networks/\"\n",
    "\n",
    "Load_Data = {}  \n",
    "for i in range(0, len(os.listdir( dataDir ))):  \n",
    "    Load_Data[i] = {}  #Dictionary for a single workspace\n",
    "\n",
    "    \n",
    "\n",
    "### lOAD THE DATA STORING THEM IN A DICTIONARY ###\n",
    "\n",
    "i = 0\n",
    "for file in os.listdir( dataDir ):\n",
    "    Load_Data[i]['File'] =  scipy.io.loadmat( dataDir+file )\n",
    "    i = i+1\n",
    "    \n",
    "\n",
    "#### Define another dictionary for storing intensity curves and classes ### \n",
    "#From each dataset, the intensity curve and classes are extracted and stored in another dictionary\n",
    "\n",
    "DATA = {}\n",
    "N_of_file =len(os.listdir( dataDir ))\n",
    "for i in range(0, N_of_file):\n",
    "        DATA[i] = {}\n",
    "\n",
    "\n",
    "\n",
    "#### Fill the dictionary ###\n",
    "\n",
    "Ncell_classified = 200 #Number of classified cells per dataset\n",
    "for i in range(0, N_of_file):\n",
    "    #Decision array for each dataset\n",
    "    DATA[i]['decision'] = Load_Data[i]['File']['decisionlist']\n",
    "    #Size of the matrix to store intensity values\n",
    "    NumofData = len(Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'])\n",
    "    Int_curve_len = len(Load_Data[4]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][0][0])\n",
    "    #Empty matrix to store all the intensity values of all the cells\n",
    "    DATA[i]['intensity'] = np.zeros((Ncell_classified,  Int_curve_len ))\n",
    "    for k in range(0,Ncell_classified): #Only the first 200 were classified\n",
    "        DATA[i]['intensity'][k,:] = Load_Data[i]['File']['RAWDATA'][0,0]['files'][0,0][0]['intensity'][k][0,0:900] #copy the first 900 values\n",
    "\n",
    "        \n",
    "### Classes array: contain values from all the data ###\n",
    "Classes=[]\n",
    "for i in range( 0, N_of_file ):\n",
    "    Classes = np.append(Classes , DATA[i]['decision']).astype(int)\n",
    "\n",
    "    \n",
    "### Intensities matrix: contain values from all the data    \n",
    "NTot_Data = N_of_file*Ncell_classified\n",
    "Intensity = np.zeros((0, DATA[0]['intensity'].shape[1]))\n",
    "for i in range( 0, N_of_file ):\n",
    "    Intensity = np.append(Intensity, DATA[i]['intensity'][:,:], axis=0)\n",
    "\n",
    "    \n",
    "### Elimate intensity profiles with NaN values and corresponding classes ###\n",
    "nan_indices = np.where(np.isnan(Intensity))\n",
    "Intensity = np.delete(Intensity,nan_indices[0], axis = 0)\n",
    "Classes = np.delete(Classes,nan_indices[0], axis = 0)\n",
    "#Redefine the total number of data\n",
    "total = len(Intensity)\n",
    "\n",
    "\n",
    "### Downsampling ###\n",
    "data1 = Intensity[:,0:896]\n",
    "step = 7\n",
    "data_downsampl = data1[:,::step]\n",
    "#Redefine len of the Intensity curves:\n",
    "Int_profile_len = len(data_downsampl[0])\n",
    "\n",
    "\n",
    "### Distribute all values randomly ###\n",
    "randseq = np.random.choice(np.arange(0,total),total,replace=False)\n",
    "#Data randomly organized\n",
    "data_downsampl = data_downsampl[randseq,:]\n",
    "Classes = Classes[randseq]\n",
    "\n",
    "######################### load file - training data ####################\n",
    "train_dt = data_downsampl[0:800,:]\n",
    "train_cl = Classes[0:800]\n",
    "X = train_dt  \n",
    "y = train_cl \n",
    "N_train_cell = len(train_dt)\n",
    "# process the data to fit in a keras CNN properly\n",
    "# input data needs to be (N, C, X, Y) - shaped where\n",
    "# N - number of samples\n",
    "# C - number of channels per sample\n",
    "# (X, Y) - sample size\n",
    "\n",
    "X = X.reshape((N_train_cell, Int_profile_len,1, 1))\n",
    "y = np_utils.to_categorical(y)\n",
    "\n",
    "\n",
    "######################### load file - testing data ####################\n",
    "#Define variables \n",
    "N_test_cell = total - N_train_cell\n",
    "#Matrix for intensity values\n",
    "test_dt = np.zeros((N_test_cell,Int_profile_len))\n",
    "#Use as testing data the last N_test_cell values within the total \n",
    "test_dt[:,:]= data_downsampl[N_train_cell:total]\n",
    "#Same process for the associated classes\n",
    "test_cl = Classes[N_train_cell:total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4e14f687387c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIntensity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "plot(Intensity[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
